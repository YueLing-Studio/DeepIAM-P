{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812534a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.read_csv(\"AR6_scenario.csv\")\n",
    "target_variables = [\n",
    "    'Emissions|CO2',\n",
    "    'Emissions|CO2|AFOLU',\n",
    "    'Emissions|CO2|Energy|Demand|Industry',\n",
    "    'Emissions|CO2|Energy|Supply|Electricity',\n",
    "    'Emissions|CO2|Energy and Industrial Processes',\n",
    "    'Final Energy|Industry|Solids|Biomass',\n",
    "    'Final Energy|Industry|Solids|Coal',\n",
    "    'Final Energy|Residential and Commercial|Solids|Coal',\n",
    "    'Primary Energy|Coal' ,\n",
    "    'Secondary Energy|Electricity|Coal' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3155f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_target.groupby(['Model', 'Scenario']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58afb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = df_target.groupby(['Model', 'Scenario'])['Variable'].nunique()  \n",
    "common_pairs_index = pair_counts[pair_counts == len(target_variables)].index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common = df_target[df_target.set_index(['Model', 'Scenario']).index.isin(common_pairs_index)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf5bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paired = df_common.set_index(['Model', 'Scenario', 'Variable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c3068",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cols = [col for col in df_target if col.isdigit()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f43e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_new_features_list = []\n",
    "\n",
    "for var in target_variables:  \n",
    "    print(f\" Starting processing '{var}'...\")  \n",
    "    var_idx = pd.MultiIndex.from_product([common_pairs_index.get_level_values('Model'),  \n",
    "                                                     common_pairs_index.get_level_values('Scenario'),  \n",
    "                                                     [var]], names=['Model', 'Scenario', 'Variable'])  \n",
    "\n",
    "    valid_var_idx = var_idx.intersection(df_paired.index)  \n",
    "    if valid_var_idx.empty:  \n",
    "        print(f\"Warning: There is no data with variable '{var}' in the common pairs. Skipping.\")  \n",
    "        continue  \n",
    "\n",
    "    var_data = df_paired.loc[valid_var_idx, year_cols]  \n",
    "    var_data = var_data.reset_index(level='Variable', drop=True)  \n",
    "    var_data = var_data.apply(pd.to_numeric, errors='coerce')  \n",
    "\n",
    "    if not year_cols:\n",
    "        feature1 = pd.Series(0.0, index=var_data.index) \n",
    "        print(f\"Warning: Variable '{var}' has no year columns in the 2020-2100 range. Feature 1 (Sum) set to 0.\")  \n",
    "    else:  \n",
    "        feature1 = var_data[year_cols].sum(axis=1, skipna=True)  \n",
    "        feature1.name = f\"{var}_sum_2020_2100\"  \n",
    "\n",
    "    if '2020' in var_data.columns and '2030' in var_data.columns:  \n",
    "        feature2 = (var_data['2030'].fillna(0) - var_data['2020'].fillna(0)) / 10  \n",
    "    else:  \n",
    "        print(f\"Warning: Variable '{var}' has no '2020' or '2030' data, Feature 2 set to 0/NaN.\")  \n",
    "        feature2 = pd.Series(np.nan, index=var_data.index) \n",
    "    feature2.name = f\"{var}_trend_2020_2030\"  \n",
    "    \n",
    "    if '2040' in var_data.columns and '2050' in var_data.columns:  \n",
    "        feature3 = (var_data['2050'].fillna(0) - var_data['2030'].fillna(0)) / 20  \n",
    "    else:  \n",
    "        print(f\"Warning: Variable '{var}' has no '2030' or '2050' data, Feature 3 set to 0/NaN.\")  \n",
    "        feature3 = pd.Series(np.nan, index=var_data.index) \n",
    "    feature3.name = f\"{var}_trend_2030_2050\" \n",
    "\n",
    "    var_features_df = pd.concat([feature1, feature2, feature3], axis=1)  \n",
    "    all_new_features_list.append(var_features_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2263e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat(all_new_features_list, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_map = df_common.groupby(['Model', 'Scenario'])['Model'].first()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = xy_map.loc[X.index]  \n",
    "print(f\"The shape of target variable y: {y.shape}\")  \n",
    "print(f\"The distribution of target variable y:\\n{y.value_counts(normalize=True)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4622329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(class_weight='balanced',random_state=42)  \n",
    "parameters = {  \n",
    "    'n_estimators': [100, 150, 200, 300], \n",
    "    'max_depth': [8, 10, 12,14],    \n",
    "    'min_samples_split': [4, 6, 8]\n",
    "}  \n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  \n",
    "print(\"Starting GridSearchCV for hyperparameter tuning...\")  \n",
    "gridsearch = GridSearchCV(classifier, parameters, cv=cv_strategy, n_jobs=-1, verbose=2, scoring='f1_weighted')  \n",
    "gridsearch.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params = gridsearch.best_params_  \n",
    "rf_best_estimator = gridsearch.best_estimator_ \n",
    "\n",
    "print(f\"\\nBest Parameters Found by GridSearchCV: {gridsearch.best_params_}\")  \n",
    "print(f\"Best cross-validated accuracy score during GridSearchCV: {gridsearch.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c38736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(rf_best_estimator, X, y, cv=cv_strategy, scoring='accuracy', n_jobs=-1)  \n",
    "\n",
    "print(f\"\\nCross-Validation Accuracy Scores for each fold: {cv_scores}\")  \n",
    "print(f\"Mean Cross-Validation Accuracy: {np.mean(cv_scores):.4f}\")  \n",
    "print(f\"Standard Deviation of Cross-Validation Accuracy: {np.std(cv_scores):.4f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea595bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining the final model on the entire dataset...\")  \n",
    "rf_classifier = RandomForestClassifier(**gridsearch.best_params_, random_state=42) \n",
    "rf_classifier.fit(X, y)  \n",
    "print(\"Final model trained successfully on all data.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1740e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = rf_classifier.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importance})\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e305f12",
   "metadata": {},
   "source": [
    "# Predict Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f3ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic = pd.read_csv(\"synthetic_scenario.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffda51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_var_counts = df_synthetic.groupby('sample_id')['Variable'].nunique()\n",
    "complete_samples = sample_var_counts[sample_var_counts == len(target_variables)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d20473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(complete_samples) < len(sample_var_counts):\n",
    "    complete_sample_ids = complete_samples.index\n",
    "    df_synthetic_filtered = df_synthetic[df_synthetic['sample_id'].isin(complete_sample_ids)]\n",
    "else:\n",
    "    df_synthetic_filtered = df_synthetic.copy()\n",
    "    complete_sample_ids = df_synthetic['sample_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6104519",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cols = [col for col in df_synthetic_filtered.columns if col.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0c791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic_indexed = df_synthetic_filtered.set_index(['sample_id', 'Variable'])\n",
    "\n",
    "all_synthetic_features_list = []\n",
    "\n",
    "for var in target_variables:\n",
    "    print(f\"Start processing variable '{var}'...\")\n",
    "    \n",
    "    var_idx = pd.MultiIndex.from_product([complete_sample_ids, [var]], \n",
    "                                        names=['sample_id', 'Variable'])\n",
    "    \n",
    "    valid_var_idx = var_idx.intersection(df_synthetic_indexed.index)\n",
    "    if valid_var_idx.empty:\n",
    "        print(f\" Warning: Variable '{var}' not found in synthetic data, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    var_data = df_synthetic_indexed.loc[valid_var_idx, year_cols]\n",
    "    var_data = var_data.reset_index(level='Variable', drop=True)\n",
    "    var_data = var_data.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    if not year_cols:\n",
    "        feature1 = pd.Series(0.0, index=var_data.index)\n",
    "        print(f\"Warning: Variable '{var}' has no year columns in the 2020-2100 range. Feature 1 (Sum) set to 0.\")\n",
    "    else:\n",
    "        feature1 = var_data[year_cols].sum(axis=1, skipna=True)\n",
    "        feature1.name = f\"{var}_sum_2020_2100\"\n",
    "    \n",
    "    if '2020' in var_data.columns and '2030' in var_data.columns:\n",
    "        feature2 = (var_data['2030'].fillna(0) - var_data['2020'].fillna(0)) / 10\n",
    "    else:\n",
    "        print(f\"Warning: Variable '{var}' missing '2020' or '2030' data, Feature 2 set to NaN.\")\n",
    "        feature2 = pd.Series(np.nan, index=var_data.index)\n",
    "    feature2.name = f\"{var}_trend_2020_2030\"\n",
    "    \n",
    "    if '2030' in var_data.columns and '2050' in var_data.columns:\n",
    "        feature3 = (var_data['2050'].fillna(0) - var_data['2030'].fillna(0)) / 20\n",
    "    else:\n",
    "        print(f\"Warning: Variable '{var}' missing '2030' or '2050' data, Feature 3 set to NaN.\")\n",
    "        feature3 = pd.Series(np.nan, index=var_data.index)\n",
    "    feature3.name = f\"{var}_trend_2030_2050\"\n",
    "    \n",
    "    var_features_df = pd.concat([feature1, feature2, feature3], axis=1)\n",
    "    all_synthetic_features_list.append(var_features_df)\n",
    "\n",
    "\n",
    "X_synthetic = pd.concat(all_synthetic_features_list, axis=1)\n",
    "\n",
    "if X_synthetic.isnull().any().any():\n",
    "    X_synthetic = X_synthetic.fillna(0)\n",
    "\n",
    "if hasattr(rf_classifier, 'feature_names_in_'):\n",
    "    expected_features = rf_classifier.feature_names_in_\n",
    "\n",
    "    \n",
    "    missing_features = set(expected_features) - set(X_synthetic.columns)\n",
    "    extra_features = set(X_synthetic.columns) - set(expected_features)\n",
    "    \n",
    "\n",
    "    X_synthetic = X_synthetic[expected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf_classifier.predict(X_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_proba = rf_classifier.predict_proba(X_synthetic)\n",
    "max_proba = np.max(prediction_proba, axis=1)\n",
    "\n",
    "\n",
    "prediction_results = pd.DataFrame({\n",
    "    'sample_id': X_synthetic.index,\n",
    "    'predicted_model': predictions,\n",
    "    'prediction_confidence': max_proba\n",
    "})\n",
    "\n",
    "print(prediction_results['predicted_model'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f06c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_synthetic_filtered.merge(prediction_results[['sample_id', 'predicted_model']], \n",
    "                                       on='sample_id', how='left')\n",
    "\n",
    "df_result = df_result.rename(columns={'predicted_model': 'Model'})\n",
    "\n",
    "cols = df_result.columns.tolist()\n",
    "if 'Model' in cols and 'Variable' in cols:\n",
    "    cols.remove('Model')\n",
    "    var_idx = cols.index('Variable')\n",
    "    cols.insert(var_idx, 'Model')\n",
    "    df_result = df_result[cols]\n",
    "\n",
    "print(df_result.head(10))\n",
    "\n",
    "\n",
    "df_result.to_csv(\"synthetic_scenario_model.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

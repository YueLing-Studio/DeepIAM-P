{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55025a0",
   "metadata": {},
   "source": [
    "# Date Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0146b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3bf1e",
   "metadata": {},
   "source": [
    "## load df_target & target_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c966344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.read_csv(\"df_target.csv\")\n",
    "target_variables = np.load('target_variables.npy', allow_pickle=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c193ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vars_set = set(target_variables)  \n",
    "num_target_vars = len(target_vars_set) \n",
    "df_grouped = df_target.groupby(['Model', 'Scenario']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e36aef",
   "metadata": {},
   "source": [
    "### Merge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3a4b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "MESSAGEix = list(set([i for i in df_target['Model'] if  'MESSAGE' in i]))\n",
    "WITCH = list(set([i for i in df_target['Model'] if  'WITCH' in i]))\n",
    "COFFEE = ['COFFEE 1.1']\n",
    "REMIND = list(set([i for i in df_target['Model'] if  'REM' in i]))\n",
    "TIA = list(set([i for i in df_target['Model'] if  'TIAM-ECN' in i]))\n",
    "POL = list(set([i for i in df_target['Model'] if  'POL' in i]))\n",
    "AIM = list(set([i for i in df_target['Model'] if  'AIM' in i]))\n",
    "IMA = list(set([i for i in df_target['Model'] if  'IMAGE' in i]))\n",
    "GCA = list(set([i for i in df_target['Model'] if  'GCA' in i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "409165b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = [MESSAGEix,WITCH,COFFEE,REMIND,TIA,POL,AIM,IMA,GCA]\n",
    "Model_names = ['MESSAGEix','WITCH','COFFEE','REMIND','TIA','POL','AIM','IMA','GCA']\n",
    "Model_List = []\n",
    "for i in Model:\n",
    "    Model_List += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_target.copy()\n",
    "df_merged = df_merged[df_merged['Model'].isin(Model_List)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3549e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {j:Model_names[i] for i in range(len(Model)) for j in Model[i]}\n",
    "df_merged['Model'] = df_merged['Model'].replace(mapping)\n",
    "df_merged = df_merged.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "819cc2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_merged.groupby(['Model', 'Scenario']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba3884",
   "metadata": {},
   "source": [
    "## Retain scenarios with all target-variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b3afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = df_merged.groupby(['Model', 'Scenario'])['Variable'].nunique()  \n",
    "common_pairs_index = pair_counts[pair_counts == len(target_variables)].index  \n",
    "print(f\"ÂÖ±Êúâ {len(common_pairs_index)} ‰∏™ (Model, Scenario) ÂØπÊã•ÊúâÂÖ®ÈÉ® {len(target_variables)} ‰∏™ÂèòÈáèÁöÑÊï∞ÊçÆ„ÄÇ\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f555841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common = df_merged[df_merged.set_index(['Model', 'Scenario']).index.isin(common_pairs_index)] \n",
    "print(f\"Á≠õÈÄâÂÖ±ÂêåÂØπÂêéÁöÑÊï∞ÊçÆÂΩ¢Áä∂: {df_common.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fb877d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_common.to_csv(\"df_common.csv\", index = False)\n",
    "df_common = pd.read_csv(\"df_common.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50c3e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paired = df_common.set_index(['Model', 'Scenario', 'Variable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390a02a",
   "metadata": {},
   "source": [
    "## Construct feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e0a8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cols = [col for col in df_target if col.isdigit()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_new_features_list = []\n",
    "\n",
    "for var in target_variables:  \n",
    "    print(f\"  Start processing Variable '{var}'...\")  \n",
    "    var_idx = pd.MultiIndex.from_product([common_pairs_index.get_level_values('Model'),  \n",
    "                                                     common_pairs_index.get_level_values('Scenario'),  \n",
    "                                                     [var]], names=['Model', 'Scenario', 'Variable'])  \n",
    "\n",
    "    valid_var_idx = var_idx.intersection(df_paired.index)  \n",
    "    if valid_var_idx.empty:  \n",
    "        print(f\"Warning: Variable '{var}' has no data in df_paired. Skipping.\")  \n",
    "        continue  \n",
    "\n",
    "    var_data = df_paired.loc[valid_var_idx, year_cols]  \n",
    "    var_data = var_data.reset_index(level='Variable', drop=True)  \n",
    "    var_data = var_data.apply(pd.to_numeric, errors='coerce')  \n",
    "\n",
    "    # feature1: the sum of 2020-2100 \n",
    "    if not year_cols:\n",
    "        feature1 = pd.Series(0.0, index=var_data.index) \n",
    "        print(f\"Warning: Variable '{var}' has no 2020-2100 year columns. Feature 1 (Sum) set to 0.\")  \n",
    "    else:  \n",
    "        feature1 = var_data[year_cols].sum(axis=1, skipna=True)  \n",
    "        feature1.name = f\"{var} Cumulative\"  \n",
    "\n",
    "    # feature2: (2030 - 2020) / 10  \n",
    "    if '2020' in var_data.columns and '2030' in var_data.columns:  \n",
    "        feature2 = (var_data['2030'].fillna(0) - var_data['2020'].fillna(0)) / 10  \n",
    "    else:  \n",
    "        print(f\"Warning: Variable '{var}' missing '2020' or '2030' data, Feature 2 set to all NaN.\")  \n",
    "        feature2 = pd.Series(np.nan, index=var_data.index) \n",
    "    feature2.name = f\"{var} 2020-2030\"  \n",
    "    \n",
    "    # feature3: (2040 - 2030) / 10  \n",
    "    if '2040' in var_data.columns and '2030' in var_data.columns:  \n",
    "        feature3 = (var_data['2040'].fillna(0) - var_data['2030'].fillna(0)) / 10  \n",
    "    else:  \n",
    "        print(f\"Warning: Variable '{var}' missing '2030' or '2040' data, Feature 3 set to all NaN.\")  \n",
    "        feature3 = pd.Series(np.nan, index=var_data.index) \n",
    "    feature3.name = f\"{var} 2030-2040\" \n",
    "\n",
    "    var_features_df = pd.concat([feature1, feature2, feature3], axis=1)  \n",
    "    all_new_features_list.append(var_features_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ef50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat(all_new_features_list, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c20f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_map = df_common.groupby(['Model', 'Scenario'])['PC_m'].first()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = xy_map.loc[X.index]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b7b46",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b4de2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb9220",
   "metadata": {},
   "source": [
    "## Fitting RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f377684",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(random_state=42)  \n",
    "parameters = {  \n",
    "    'n_estimators': [100, 150, 200, 300], \n",
    "    'max_depth': [10, 12, 14],    \n",
    "    'min_samples_split': [4, 6, 8]\n",
    "}  \n",
    "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  \n",
    "print(\"Starting GridSearchCV for hyperparameter tuning...\")  \n",
    "gridsearch = GridSearchCV(classifier, parameters, cv=cv_strategy, n_jobs=-1, verbose=2, scoring='accuracy')  \n",
    "gridsearch.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895dadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params = gridsearch.best_params_  \n",
    "rf_best_estimator = gridsearch.best_estimator_ \n",
    "\n",
    "print(f\"\\nBest Parameters Found by GridSearchCV: {gridsearch.best_params_}\")  \n",
    "print(f\"Best cross-validated accuracy score during GridSearchCV: {gridsearch.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(rf_best_estimator, X, y, cv=cv_strategy, scoring='accuracy', n_jobs=-1)  \n",
    "\n",
    "print(f\"\\nCross-Validation Accuracy Scores for each fold: {cv_scores}\")  \n",
    "print(f\"Mean Cross-Validation Accuracy: {np.mean(cv_scores):.4f}\")  \n",
    "print(f\"Standard Deviation of Cross-Validation Accuracy: {np.std(cv_scores):.4f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining the final model on the entire dataset...\")  \n",
    "rf_classifier = RandomForestClassifier(**gridsearch.best_params_, random_state=42) \n",
    "rf_classifier.fit(X, y)  \n",
    "print(\"Final model trained successfully on all data.\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a489d67",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed0e759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_path = './feature_importance_20_to_40'\n",
    "os.makedirs(feature_importance_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab96b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = rf_classifier.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importance})\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9865850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df.to_csv(os.path.join(feature_importance_path, 'Feature_Importance.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f72db",
   "metadata": {},
   "source": [
    "### Importance rank by feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95c19c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_target_variables_from_features(feature_names):\n",
    "    target_vars = set()\n",
    "    suffixes = [' Cumulative', ' 2020-2030', ' 2030-2040']\n",
    "    \n",
    "    for feature in feature_names:\n",
    "        for suffix in suffixes:\n",
    "            if feature.endswith(suffix):\n",
    "                target_var = feature[:-len(suffix)]\n",
    "                target_vars.add(target_var)\n",
    "                break\n",
    "    \n",
    "    return sorted(list(target_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd269e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variables_from_features = extract_target_variables_from_features(feature_importance_df['Feature'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_feature_name(name, max_chars_per_line=20):\n",
    "\n",
    "    if len(name) <= max_chars_per_line:\n",
    "        return name\n",
    "    \n",
    "\n",
    "    separators = [\n",
    "        ('|', True),  \n",
    "        ('_', True), \n",
    "        (' ', False),\n",
    "        ('-', True)  \n",
    "    ]\n",
    "    \n",
    "    best_split = None\n",
    "    best_score = float('inf')\n",
    "    \n",
    "    for sep, keep_separator in separators:\n",
    "        if sep in name:\n",
    "            parts = name.split(sep)\n",
    "            \n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            \n",
    "            for split_idx in range(2, len(parts)):\n",
    "        \n",
    "                first_parts = parts[:split_idx]\n",
    "                second_parts = parts[split_idx:]\n",
    "                \n",
    "                first_line = sep.join(first_parts)\n",
    "                second_line = sep.join(second_parts)\n",
    "                \n",
    "\n",
    "                if len(second_line) <= max_chars_per_line:\n",
    "    \n",
    "                    if len(first_line) <= max_chars_per_line:\n",
    "\n",
    "                        score = abs(len(first_line) - len(second_line))  \n",
    "                    else:\n",
    "                        score = len(first_line) - max_chars_per_line + 100\n",
    "                    \n",
    "                    if score < best_score:\n",
    "                        best_score = score\n",
    "                        best_split = (first_line, second_line, sep, keep_separator)\n",
    "    \n",
    "    if best_split:\n",
    "        first_line, second_line, sep, keep_separator = best_split\n",
    "        if keep_separator:\n",
    "            return first_line + sep + '\\n' + second_line\n",
    "        else:\n",
    "            return first_line + '\\n' + second_line\n",
    "    \n",
    "    for sep, keep_separator in separators:\n",
    "        if sep in name:\n",
    "            parts = name.split(sep)\n",
    "            \n",
    "            if len(parts) >= 2:\n",
    "                first_line = sep.join(parts[:2])\n",
    "                second_line = sep.join(parts[2:])\n",
    "                \n",
    "                if len(second_line) <= max_chars_per_line * 1.5: \n",
    "                    if keep_separator:\n",
    "                        return first_line + sep + '\\n' + second_line\n",
    "                    else:\n",
    "                        return first_line + '\\n' + second_line\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92154ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wrap_feature_name(name, max_chars_per_line=20):\n",
    "\n",
    "    if len(name) <= max_chars_per_line:\n",
    "        return name\n",
    "    \n",
    "    # Ê†πÊçÆÊÄªÈïøÂ∫¶ÂÜ≥ÂÆöÁõÆÊ†áË°åÊï∞\n",
    "    total_length = len(name)\n",
    "    if total_length <= max_chars_per_line * 2:\n",
    "        target_lines = 2\n",
    "    elif total_length <= max_chars_per_line * 3:\n",
    "        target_lines = 3\n",
    "    else:\n",
    "        target_lines = 3  \n",
    "    \n",
    "    separators = ['|', '_', ' ', '-']\n",
    "    \n",
    "    def split_into_lines(text, target_lines, separator):\n",
    "        if separator not in text:\n",
    "            return None\n",
    "            \n",
    "        parts = text.split(separator)\n",
    "        if len(parts) < target_lines:\n",
    "            return None\n",
    "        \n",
    "        parts_per_line = len(parts) // target_lines\n",
    "        extra_parts = len(parts) % target_lines\n",
    "        \n",
    "        lines = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for line_num in range(target_lines):\n",
    "            current_line_parts = parts_per_line + (1 if line_num < extra_parts else 0)\n",
    "            end_idx = start_idx + current_line_parts\n",
    "            line_parts = parts[start_idx:end_idx]\n",
    "            line_text = separator.join(line_parts)\n",
    "            lines.append(line_text)\n",
    "            \n",
    "            start_idx = end_idx\n",
    "        \n",
    "        return lines\n",
    "    \n",
    "    def evaluate_split_quality(lines, max_chars):\n",
    "\n",
    "        if not lines:\n",
    "            return float('inf')\n",
    "        \n",
    "        if any(len(line.strip()) == 0 for line in lines):\n",
    "            return float('inf')\n",
    "        \n",
    "        max_line_length = max(len(line) for line in lines)\n",
    "        over_limit_penalty = max(0, max_line_length - max_chars) * 100\n",
    "        \n",
    "        avg_length = sum(len(line) for line in lines) / len(lines)\n",
    "        variance = sum((len(line) - avg_length) ** 2 for line in lines) / len(lines)\n",
    "        \n",
    "        return over_limit_penalty + variance\n",
    "    \n",
    "    best_result = None\n",
    "    best_score = float('inf')\n",
    "    \n",
    "    for separator in separators:\n",
    "        for lines_to_try in [target_lines, max(2, target_lines - 1)]:\n",
    "            result = split_into_lines(name, lines_to_try, separator)\n",
    "            if result:\n",
    "                score = evaluate_split_quality(result, max_chars_per_line)\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_result = result\n",
    "    \n",
    "    if best_result:\n",
    "        return '\\n'.join(best_result)\n",
    "    \n",
    "    for separator in separators:\n",
    "        if separator in name:\n",
    "            parts = name.split(separator)\n",
    "            if len(parts) >= 2:\n",
    "                mid = len(parts) // 2\n",
    "                line1 = separator.join(parts[:mid])\n",
    "                line2 = separator.join(parts[mid:])\n",
    "                return line1 + '\\n' + line2\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distinct_colors(n):\n",
    "    if n <= 10:\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, 10))[:n]\n",
    "    elif n <= 20:\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, 20))[:n]\n",
    "    else:\n",
    "        colors = plt.cm.hsv(np.linspace(0, 1, n))\n",
    "    \n",
    "    return [mcolors.rgb2hex(color) for color in colors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_n_features_optimized(feature_importance_df, n, custom_text_colors=None, custom_bar_colors=None, \n",
    "                                  max_chars_per_line=30, ylabel_position=(0.02, 0.5), ylabel_rotation=0,\n",
    "                                  font_size=8, bar_height=0.6, fig_height=None, fig_width=12):\n",
    "    \n",
    "    top_n_df = feature_importance_df.nlargest(n, 'Importance').sort_values('Importance', ascending=True)\n",
    "    \n",
    "    suffixes = [' Cumulative', ' 2020-2030', ' 2030-2040']\n",
    "    \n",
    "    def extract_target_variable_and_suffix(feature_name):\n",
    "        for suffix in suffixes:\n",
    "            if feature_name.endswith(suffix):\n",
    "                target_var = feature_name[:-len(suffix)]\n",
    "                return target_var, suffix\n",
    "        return feature_name, None  \n",
    "    \n",
    "    top_n_df = top_n_df.copy()\n",
    "    target_suffix_pairs = top_n_df['Feature'].apply(extract_target_variable_and_suffix)\n",
    "    top_n_df['Target_Variable'] = [pair[0] for pair in target_suffix_pairs]\n",
    "    top_n_df['Suffix'] = [pair[1] for pair in target_suffix_pairs]\n",
    "    \n",
    "    unique_targets = top_n_df['Target_Variable'].unique()\n",
    "    unique_suffixes = [suffix for suffix in suffixes if suffix in top_n_df['Suffix'].values]\n",
    "    \n",
    "    text_default_colors = generate_distinct_colors(len(unique_targets))\n",
    "    text_color_mapping = {target: text_default_colors[i] for i, target in enumerate(unique_targets)}\n",
    "    \n",
    "    if custom_text_colors:\n",
    "        for target, color in custom_text_colors.items():\n",
    "            if target in text_color_mapping:\n",
    "                text_color_mapping[target] = color\n",
    "    \n",
    "    default_bar_colors = {\n",
    "        ' Cumulative': '#FAC7DA',      \n",
    "        ' 2020-2030': '#D1E3FF',    \n",
    "        ' 2030-2040': '#FFF4F5' \n",
    "    }\n",
    "    \n",
    "    bar_color_mapping = default_bar_colors.copy()\n",
    "    \n",
    "    if custom_bar_colors:\n",
    "        for suffix, color in custom_bar_colors.items():\n",
    "            if suffix in bar_color_mapping:\n",
    "                bar_color_mapping[suffix] = color\n",
    "    \n",
    "    text_colors = [text_color_mapping[target] for target in top_n_df['Target_Variable']]\n",
    "    bar_colors = [bar_color_mapping.get(suffix, '#888888') for suffix in top_n_df['Suffix']]\n",
    "    \n",
    "    wrapped_feature_names = [wrap_feature_name(name, max_chars_per_line) for name in top_n_df['Feature']]\n",
    "    \n",
    "    if fig_height is None:\n",
    "        base_height_per_feature = max(0.3, bar_height * 0.8)  \n",
    "        font_factor = font_size / 10.0 \n",
    "        calculated_height = max(6, n * base_height_per_feature * font_factor + 2) \n",
    "        fig_height = calculated_height\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "    \n",
    "    bars = ax.barh(range(len(top_n_df)), top_n_df['Importance'], \n",
    "                   color=bar_colors, height=bar_height)\n",
    "    \n",
    "    ax.set_yticks(range(len(wrapped_feature_names)))\n",
    "    ax.set_yticklabels(wrapped_feature_names, fontsize=font_size, weight='bold')\n",
    "\n",
    "    for i, target_var in enumerate(top_n_df['Target_Variable']):\n",
    "        ax.get_yticklabels()[i].set_color(text_color_mapping[target_var])\n",
    "    \n",
    "    ax.set_xlabel('Feature Importance', fontsize=18, weight='bold')\n",
    "    ax.tick_params(axis='x', labelsize=15)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)      \n",
    "    ax.spines['right'].set_visible(False)   \n",
    "    ax.spines['left'].set_visible(True)      \n",
    "    ax.spines['bottom'].set_visible(True)  \n",
    "    \n",
    "    ax.spines['left'].set_linewidth(2)      \n",
    "    ax.spines['bottom'].set_linewidth(2)   \n",
    "    \n",
    "    fig.text(ylabel_position[0], ylabel_position[1], 'Features', \n",
    "             fontsize=18, weight='bold', rotation=ylabel_rotation,\n",
    "             ha='center', va='center', transform=fig.transFigure)\n",
    "    \n",
    "    text_legend_elements = [plt.Rectangle((0,0),1,1, facecolor=text_color_mapping[target], \n",
    "                                        label=f'Text: {target}') for target in unique_targets]\n",
    "    \n",
    "    suffix_labels = {\n",
    "        ' Cumulative': 'Bar: Sum (2020-2100)',\n",
    "        ' 2020-2030': 'Bar: Trend (2020-2030)',\n",
    "        ' 2030-2040': 'Bar: Trend (2030-2040)',\n",
    "    }\n",
    "    \n",
    "    bar_legend_elements = [plt.Rectangle((0,0),1,1, facecolor=bar_color_mapping[suffix], \n",
    "                                       label=suffix_labels[suffix]) \n",
    "                          for suffix in unique_suffixes]\n",
    "    \n",
    "    all_legend_elements = text_legend_elements + bar_legend_elements\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(feature_importance_path, f'Compact_Top_{n}_Feature_Importance.png'), dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return text_color_mapping, bar_color_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec53f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_features = 20\n",
    "\n",
    "custom_text_colors = {\n",
    "    'Emissions|CO2': '#664B3E',  #\n",
    "    'Emissions|CO2|AFOLU': '#BF7744', #\n",
    "    \n",
    "    'Final Energy|Industry|Solids|Coal': '#A59ACA', #\n",
    "    \n",
    "    'Emissions|CO2|Energy|Supply|Electricity': '#F1BD3F',#\n",
    "    'Emissions|CO2|Energy|Demand|Industry': '#CFB697', #\n",
    "    \n",
    "    'Final Energy|Residential and Commercial|Solids|Coal': '#7A7B78',  #\n",
    "    'Secondary Energy|Electricity|Coal': '#FFBE7A',  #\n",
    "    'Final Energy|Industry|Solids|Biomass': \"#9BBF8A\", #\n",
    "    'Emissions|CO2|Energy and Industrial Processes': '#F07874', \n",
    "    'Primary Energy|Coal': '#C82423', #\n",
    "    'Primary Energy|Nuclear': '#A14F6C', #\n",
    "    'Final Energy|Residential and Commercial|Liquids': '#87C0CA',  #\n",
    "    'Carbon Sequestration|CCS|Biomass': '#45465E', # \n",
    "    'Primary Energy|Wind': '#80B1D2', #\n",
    "    'Primary Energy|Hydro': '#354E6B', #\n",
    "    'Carbon Sequestration|CCS': '#CA9C91', #\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d0030d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bar_colors = {\n",
    "        ' Cumulative': '#DCC1AD',      \n",
    "        ' 2020-2030': '#A4ABD6',    \n",
    "        ' 2030-2040': '#B26D50',      \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19136223",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mapping, bar_mapping = plot_top_n_features_optimized(\n",
    "    feature_importance_df, \n",
    "    n_top_features,\n",
    "    custom_text_colors, \n",
    "    custom_bar_colors, \n",
    "    max_chars_per_line=30,\n",
    "    font_size=9,          \n",
    "    bar_height=0.8,       \n",
    "    ylabel_position=(0.1, 1.05),  \n",
    "    ylabel_rotation=0,  \n",
    "    fig_height=20, \n",
    "    fig_width=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2843d3",
   "metadata": {},
   "source": [
    "### Importance rank by temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6bfcc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "Importance_of_2020_2030 = []\n",
    "Importance_of_2030_2040 = []\n",
    "Importance_of_Sum = []\n",
    "\n",
    "for i in range(feature_importance_df.shape[0]):\n",
    "    if '2020-2030' in feature_importance_df.iloc[i]['Feature']:\n",
    "        Importance_of_2020_2030.append(feature_importance_df.iloc[i]['Importance'])\n",
    "    elif '2030-2040' in feature_importance_df.iloc[i]['Feature']:\n",
    "        Importance_of_2030_2040.append(feature_importance_df.iloc[i]['Importance'])\n",
    "    else:\n",
    "        Importance_of_Sum.append(feature_importance_df.iloc[i]['Importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def create_donut_chart_feature_importance(feature_importance_df, \n",
    "                                        inner_radius=0.4,\n",
    "                                        fig_width=10, \n",
    "                                        fig_height=8,\n",
    "                                        title_fontsize=16,\n",
    "                                        label_fontsize=12,\n",
    "                                        legend_fontsize=11,\n",
    "                                        save_path=None,\n",
    "                                        font_path=None):\n",
    "    \n",
    "    if font_path and os.path.exists(font_path):\n",
    "        font_prop = fm.FontProperties(fname=font_path)\n",
    "        plt.rcParams['font.family'] = font_prop.get_name()\n",
    "    \n",
    "    Importance_of_2020_2030 = []\n",
    "    Importance_of_2030_2040 = []\n",
    "    Importance_of_Sum = []\n",
    "    \n",
    "    for i in range(feature_importance_df.shape[0]):\n",
    "        if '2020-2030' in feature_importance_df.iloc[i]['Feature']:\n",
    "            Importance_of_2020_2030.append(feature_importance_df.iloc[i]['Importance'])\n",
    "        elif '2030-2040' in feature_importance_df.iloc[i]['Feature']:\n",
    "            Importance_of_2030_2040.append(feature_importance_df.iloc[i]['Importance'])\n",
    "        else:\n",
    "            Importance_of_Sum.append(feature_importance_df.iloc[i]['Importance'])\n",
    "    \n",
    "    cumulative_importance = sum(Importance_of_Sum)\n",
    "    period_2020_2030 = sum(Importance_of_2020_2030)\n",
    "    period_2030_2040 = sum(Importance_of_2030_2040)\n",
    "    \n",
    "    values = [cumulative_importance, period_2020_2030, period_2030_2040]\n",
    "    categories = ['Cumulative', '2020-2030', '2030-2040']\n",
    "    colors = ['#DCC1AD', '#A4ABD6', '#B26D50']\n",
    "    \n",
    "    print(f\"Cumulative: {cumulative_importance:.4f} ({len(Importance_of_Sum)} features)\")\n",
    "    print(f\"2020-2030: {period_2020_2030:.4f} ({len(Importance_of_2020_2030)} features)\")\n",
    "    print(f\"2030-2040: {period_2030_2040:.4f} ({len(Importance_of_2030_2040)} features)\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "    \n",
    "    wedges, texts, autotexts = ax.pie(values, \n",
    "                                      labels=None,  \n",
    "                                      colors=colors,\n",
    "                                      autopct='%1.1f%%',\n",
    "                                      startangle=90,\n",
    "                                      textprops={'fontsize': label_fontsize, 'fontweight': 'bold'},\n",
    "                                      pctdistance=0.85)\n",
    "    \n",
    "    centre_circle = plt.Circle((0, 0), inner_radius, fc='white', ec='lightgray', linewidth=2)\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "    \n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_fontsize(fontsize=22)\n",
    "    \n",
    "    label_radius = inner_radius * 0.7 \n",
    "    \n",
    "    for i, (wedge, category) in enumerate(zip(wedges, categories)):\n",
    "\n",
    "        theta1, theta2 = wedge.theta1, wedge.theta2\n",
    "        mid_angle = (theta1 + theta2) / 2\n",
    "\n",
    "        angle_rad = np.radians(mid_angle)\n",
    "        x = label_radius * np.cos(angle_rad)\n",
    "        y = label_radius * np.sin(angle_rad)\n",
    "\n",
    "    ax.set_title('Feature Importance by Category', \n",
    "                fontsize=title_fontsize, \n",
    "                fontweight='bold',\n",
    "                pad=20)\n",
    "\n",
    "    legend_labels = categories\n",
    "    \n",
    "    ax.legend(wedges, legend_labels,\n",
    "              title=\"Feature Type\",\n",
    "              title_fontsize=legend_fontsize + 1,\n",
    "              fontsize=legend_fontsize,\n",
    "              loc=\"center left\",\n",
    "              bbox_to_anchor=(1, 0, 0.5, 1),\n",
    "              frameon=False,\n",
    "              fancybox=False,\n",
    "              shadow=False)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight', facecolor='white')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return values, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, categories = create_donut_chart_feature_importance(\n",
    "    feature_importance_df,\n",
    "    inner_radius=0.7,\n",
    "    fig_width=10,\n",
    "    fig_height=8,\n",
    "    title_fontsize=16,\n",
    "    label_fontsize=12,\n",
    "    legend_fontsize=11,\n",
    "    save_path=os.path.join(feature_importance_path, 'Donut_Chart_Feature_Importance_nontext.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e2406",
   "metadata": {},
   "source": [
    "### Importance rank by variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f204ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = [\"_sum_2020_2100\", \"_trend_2020_2030\", \"_trend_2030_2040\"]  \n",
    "\n",
    "def get_original_variable(feature_name, base_variables): \n",
    "    for base_var in base_variables:  \n",
    "        if feature_name.startswith(base_var + \"_\"):   \n",
    "            suffix_part = feature_name[len(base_var):]  \n",
    "            if suffix_part in suffixes:  \n",
    "                 return base_var  \n",
    "    print(f\"Warning: Could not map feature '{feature_name}' to a base variable.\")  \n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9dc6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df['Original_Variable'] = feature_importance_df['Feature'].apply(  \n",
    "    lambda x: get_original_variable(x, target_variables)  \n",
    ")  \n",
    "unmapped_features = feature_importance_df[feature_importance_df['Original_Variable'].isnull()]  \n",
    "if not unmapped_features.empty:  \n",
    "    print(\"\\nCould not map the following features:\")  \n",
    "    print(unmapped_features)  \n",
    "\n",
    "feature_importance_df.dropna(subset=['Original_Variable'], inplace=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374caf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_importance = feature_importance_df.groupby('Original_Variable')['Importance'].sum()  \n",
    "aggregated_importance_df = aggregated_importance.reset_index()  \n",
    "aggregated_importance_df.rename(columns={'Importance': 'Aggregated_Importance'}, inplace=True)  \n",
    "aggregated_importance_df = aggregated_importance_df.sort_values('Aggregated_Importance', ascending=False)  \n",
    "print(\"\\nAggregated Feature Importance (Top 10):\")  \n",
    "print(aggregated_importance_df.head(10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def create_pie_chart_feature_importance(feature_importance_df, \n",
    "                                       inner_radius=0.4,  \n",
    "                                       explode_distance=0.05, \n",
    "                                       show_percentages=True,  \n",
    "                                       fig_width=10, \n",
    "                                       fig_height=8,\n",
    "                                       title_fontsize=16,\n",
    "                                       label_fontsize=12,\n",
    "                                       legend_fontsize=11,\n",
    "                                       save_path=None,\n",
    "                                       font_path=None):\n",
    "\n",
    "    \n",
    "    if font_path and os.path.exists(font_path):\n",
    "        font_prop = fm.FontProperties(fname=font_path)\n",
    "        plt.rcParams['font.family'] = font_prop.get_name()\n",
    "    \n",
    "    Importance_of_PrimaryEnergy = []\n",
    "    Importance_of_SecondaryEnergy = []\n",
    "    Importance_of_Emissions = []\n",
    "    Importance_of_FinalEnergy = []\n",
    "    Importance_of_CarbonSequestration = []\n",
    "    \n",
    "    for i in range(feature_importance_df.shape[0]):\n",
    "        if 'Primary Energy' in feature_importance_df.iloc[i]['Feature']:\n",
    "            Importance_of_PrimaryEnergy.append(feature_importance_df.iloc[i]['Importance'])\n",
    "        elif 'Secondary Energy' in feature_importance_df.iloc[i]['Feature']:\n",
    "            Importance_of_SecondaryEnergy.append(feature_importance_df.iloc[i]['Importance'])\n",
    "        elif 'Emissions' in feature_importance_df.iloc[i]['Feature']:\n",
    "            Importance_of_Emissions.append(feature_importance_df.iloc[i]['Importance'])\n",
    "        elif 'Final Energy' in feature_importance_df.iloc[i]['Feature']:\n",
    "            Importance_of_FinalEnergy.append(feature_importance_df.iloc[i]['Importance'])\n",
    "            \n",
    "        else:\n",
    "            Importance_of_CarbonSequestration.append(feature_importance_df.iloc[i]['Importance'])\n",
    "\n",
    "    PrimaryEnergy = sum(Importance_of_PrimaryEnergy)\n",
    "    SecondaryEnergy = sum(Importance_of_SecondaryEnergy)\n",
    "    Emissions = sum(Importance_of_Emissions)\n",
    "    FinalEnergy = sum(Importance_of_FinalEnergy)\n",
    "    CarbonSequestration = sum(Importance_of_CarbonSequestration)\n",
    "    \n",
    "    values = [PrimaryEnergy, SecondaryEnergy, Emissions, FinalEnergy, CarbonSequestration]\n",
    "    categories = ['Primary Energy', 'Secondary Energy', 'Emissions', 'Final Energy', 'Carbon Sequestration']\n",
    "    colors = ['#AA9687', '#DF9E73', '#C5817C', '#A4A5A6', '#71A4BA']\n",
    "    \n",
    "    explode = [explode_distance] * len(values) if explode_distance > 0 else None\n",
    "\n",
    "    if explode_distance > 0:\n",
    "        total_value = sum(values)\n",
    "        proportions = [v/total_value for v in values]\n",
    "        explode = [explode_distance * (1.5 - prop) for prop in proportions]\n",
    "\n",
    "    # ÊâìÂç∞ÁªüËÆ°‰ø°ÊÅØ\n",
    "    print(\"üìä Feature Importance by Variable:\")\n",
    "    print(f\"Primary Energy: {PrimaryEnergy:.4f} ({len(Importance_of_PrimaryEnergy)} features)\")\n",
    "    print(f\"Secondary Energy: {SecondaryEnergy:.4f} ({len(Importance_of_SecondaryEnergy)} features)\")\n",
    "    print(f\"Emissions: {Emissions:.4f} ({len(Importance_of_Emissions)} features)\")\n",
    "    print(f\"FinalEnergy: {FinalEnergy:.4f} ({len(Importance_of_FinalEnergy)} features)\")\n",
    "    print(f\"CarbonSequestration: {CarbonSequestration:.4f} ({len(Importance_of_CarbonSequestration)} features)\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "    autopct_setting = '%1.1f%%' if show_percentages else None\n",
    "\n",
    "    if show_percentages:\n",
    "        pie_result = ax.pie(values, \n",
    "                           labels=None,  \n",
    "                           colors=colors,\n",
    "                           autopct=autopct_setting,\n",
    "                           startangle=90,\n",
    "                           explode=explode,  \n",
    "                           textprops={'fontsize': label_fontsize, 'fontweight': 'bold'},\n",
    "                           pctdistance=0.85)\n",
    "        wedges, texts, autotexts = pie_result\n",
    "    else:\n",
    "        pie_result = ax.pie(values, \n",
    "                           labels=None, \n",
    "                           colors=colors,\n",
    "                           autopct=autopct_setting,  \n",
    "                           startangle=90,\n",
    "                           explode=explode,  \n",
    "                           textprops={'fontsize': label_fontsize, 'fontweight': 'bold'})\n",
    "        wedges, texts = pie_result\n",
    "        autotexts = None  \n",
    "    \n",
    "    if show_percentages and autotexts:\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontweight('bold')\n",
    "            autotext.set_fontsize(fontsize=22)\n",
    "\n",
    "    for i, (wedge, category) in enumerate(zip(wedges, categories)):\n",
    "\n",
    "        theta1, theta2 = wedge.theta1, wedge.theta2\n",
    "        mid_angle = (theta1 + theta2) / 2\n",
    "        proportion = values[i] / sum(values) * 100\n",
    "\n",
    "    ax.set_title('Feature Importance by Category', \n",
    "                fontsize=title_fontsize, \n",
    "                fontweight='bold',\n",
    "                pad=20)\n",
    "    \n",
    "    legend_labels = categories\n",
    "    \n",
    "    ax.legend(wedges, legend_labels,\n",
    "              title=\"Feature Type\",\n",
    "              title_fontsize=legend_fontsize + 1,\n",
    "              fontsize=legend_fontsize,\n",
    "              loc=\"center left\",\n",
    "              bbox_to_anchor=(1, 0, 0.5, 1),\n",
    "              frameon=False,\n",
    "              fancybox=False,\n",
    "              shadow=False)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight', facecolor='white')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return values, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, categories = create_pie_chart_feature_importance(\n",
    "    feature_importance_df,\n",
    "    explode_distance=0.01,\n",
    "    show_percentages=False, \n",
    "    save_path=os.path.join(feature_importance_path, 'importance aggregate by variable.png')\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
